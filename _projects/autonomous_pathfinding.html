---
layout: projects_layout
desc: ""
link_proj: true
title: "Autonomous Pathfinding in Simulated Environment (Publication)"
desc: "Autonomous Pathfinding in a simulated game engine environment. Designed an algorithm which used in-game stereo camera input and Deep Learning based depth estimation technique for obstacle detection and navigation. [Published at International Conference on Artificial Intelligence and Robotics, 2017]"
pdate: "Aug, 2016 - Dec, 17"
project_pos: 1
---
<style type="text/css">

figcaption::before {
	content: "Figure " counter(figure) ". ";
}
</style>

<p class="summary"><em>Bachelor project carried out with Dr. O.P. Verma during the final year of my studies at Delhi Technological University (with 3 other classmates). <a class="tooltip" href="https://dl.acm.org/citation.cfm?id=3175621" target="_blank">Published at ICRAI, 2017</a></em></a></em></p>

<h3>Motivation</h3>

<p></p>
<p>During this project we explored the viability of a camera based approach for navigation, without relying on expensive hardware.</p>
<p>The motivation was two fold:
<ol>
<li>Humans <em>do not</em> have a specialized hardware like <em>LIDAR</em> for sensing our environment, we get by with our eyes. Logically the robots we design should do so too.</li>
<li>In comparison to cameras, <em>LIDAR</em> is expensive in terms of manufacturing as well as operation</li>
</ol>
</p>

<h3>Approach</h3>

<p>We had the basic idea to use a <em>CNN</em> (Convolutional Neural Network) to extract information from our scene and interface its output with a path planner. For the project's scope, we decided to generate a scene depth map from the CNN, and use it for path planning.</p>

<p>Testing our approach with actual vehicles was not viable due to financial & safety reasons. Real world datasets available were not large enough to train a deep neural network, and making one would have been too expensive</p>

<p>Due to these constraints we implemented our autonomous agent in a simulation (made using <a class="tooltip" target="_blank" href="https://www.panda3d.org/">Panda3D</a>). We used <a target="_blank" class="tooltip" href="https://shapenet.org/">Shapenet</a> (3D object database) for obstacles in the environment.  </p>

<h3> Scene Depth Estimation </h3>
<figure>
<img src="/assets/dataset.png">
<figcaption>Sample images from Dataset: Left Camera, Right Camera, and Scene Depth Ground Truth</figcaption>
</figure>

<p>We used Shapenet to artificially generate a dataset for scene depth estimation. We selected stereo image input instead of mono, so that more information about scene depth reaches the network. (See: <a class="tooltip" href="https://en.wikipedia.org/wiki/Stereopsis" target="_blank">Stereopsis</a>)</p>

<figure>
<img src="/assets/depthmap2.png">
<figcaption>Performance of the CNN on the depth estimation task</figcaption>
</figure>

<p>Armed with our cheaply generated dataset, we trained a <em>CNN</em> which produced a depth map of the scene in view given stereo input.</p>

<h3>Path Planning</h3>
<figure>
<div class="grid50">
<img src="/assets/environment.png">
<img src="/assets/godview.png">
</div>
<figcaption>Our environment simulation and agent in action. The agent did not have access to Camera 0, that was only for our observation. The agent "saw" throught the left and right cameras, which were interfaced with the CNN. At all times the agent new which block it was at, and the coordinates of the destination block.</figcaption>
</figure>

<p>The path planner discretized the environment into 1x1 blocks and used A* algorithm to search for the destination block. Moves allowed for the agent: move forward by 1 cube and rotate by 45 degrees.</p>

<p>For short term path planning (going to the next cube) we used the CNN on the in game camera input for obstacle detection. If depth of the scene in front of the agent was less than a certain threshhold, the path was deemed unsafe.</p>

<p>We evaluated our agent on different A* heuristics for pathfinding and baselined it with human performance on same task. For humans, the overview camera was hidden and they were presented with a direction prompt about the destination block, as well as a marker on the desination.</p>

<h3>Conclusion & Next Steps</h3>
<p>We were able to demonstrate that navigation without using specialized hardware could be possible with applications in small robots and vehicles. However, using heuristics in real world scenarios would not scale well. Thus, integrating the CNN with a SLAM algorithm would be the next step to test out this approach in actual vehicles.</p>

<p>Our CNN did not understand higher level constructs like objects, it was only trained on the low level task of estimating pixel depth. An approach to train the network for the end to end navigation task wtihout a path planning algorithms, is another future direction of this reseach.</p>

<p>Our paper regarding this project was accepted at <em>International Conference on Robotics and Artificial Intelligence</em> and published in December 2017. Check out our code <a href="https://github.com/t4nuj/simulated-pathfinder" class="tooltip" target="_blank">here</a>.</p>